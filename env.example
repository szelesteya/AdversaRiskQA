# OpenAI API Configuration
# Required: Your OpenAI API key for evaluation scripts
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Adversality Evaluation Configuration
# Optional: Model used as judge for adversality evaluation
# Default: gpt-5-mini
ADVERSALITY_JUDGE_MODEL=gpt-5-mini

# Factuality Evaluation Configuration
# Optional: Model for extracting facts from responses
# Default: gpt-5
FACT_EXTRACTION_MODEL=gpt-5

# Optional: Model for validating facts with web search
# Default: gpt-5-mini-2025-08-07
FACT_VALIDATION_MODEL=gpt-5-mini-2025-08-07

# Optional: Directory path for output files from factuality evaluation
# Default: data
DATA_LIBRARY=data

# Optional: Maximum output tokens for factuality evaluation
# Default: 2048
FACT_MAX_OUTPUT_TOKENS=2048

# Note: The execute_prompts.py script does not require any environment variables.
# It uses command-line arguments and either native vLLM or a Docker container.
# If using Docker with Hugging Face models, set HUGGING_FACE_HUB_TOKEN in your shell
# or Docker environment (not in this .env file, as it's passed to the container).

