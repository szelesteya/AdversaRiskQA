{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b95ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa43a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb82a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "FACT_EXTRACTION_MODEL = os.environ.get(\"FACT_EXTRACTION_MODEL\", \"gpt-5\")\n",
    "FACT_VALIDATION_MODEL = os.environ.get(\"FACT_VALIDATION_MODEL\", \"gpt-5-mini\")\n",
    "DEBUG_MODE = False\n",
    "INDIVIDUAL_FACTS_PROMPT = \"\"\"\n",
    "You are step 1 of the SAFE factuality pipeline. Break the assistant response into\n",
    "distinct, verifiable facts. Keep each fact short, avoid duplication, and skip any\n",
    "hedging or speculation. Only extract facts that explicitly appear in the response.\n",
    "Respond using the provided schema.\n",
    "\"\"\".strip()\n",
    "FACT_VALIDATION_PROMPT = \"\"\"\n",
    "You are a SAFE-inspired factuality judge. For every fact you receive, validate it against your knowledge:\n",
    "- Determine if the fact is correct based on reliable, verifiable information.\n",
    "- Prefer high-authority sources in your reasoning (government, academic, established media).\n",
    "- Mark a fact correct only when you can confidently verify it with authoritative sources.\n",
    "- When evidence is missing, ambiguous, or contradictory, mark the fact incorrect.\n",
    "- For each fact, provide at least one source with title and URL that supports or refutes it.\n",
    "- Summarize the reasoning clearly, including specific details from the source.\n",
    "Return your assessment using the FactCheckResponse schema with complete citations.\n",
    "\"\"\".strip()\n",
    "DATA_LIBRARY = Path(os.environ.get(\"DATA_LIBRARY\", \"data\")).resolve()\n",
    "MAX_OUTPUT_TOKENS = int(os.environ.get(\"FACT_MAX_OUTPUT_TOKENS\", \"2048\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d7b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualFactsResponse(BaseModel):\n",
    "    \"\"\"Response model for extracting individual facts from a model response.\n",
    "\n",
    "    This represents Step 1 of the SAFE factuality pipeline: breaking down\n",
    "    a response into atomic, verifiable facts.\n",
    "    \"\"\"\n",
    "\n",
    "    facts: list[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Distinct, non-overlapping factual statements extracted from the model's \"\n",
    "            \"response. Each fact should be: (1) atomic and self-contained, (2) verifiable \"\n",
    "            \"through external sources, (3) free of hedging language like 'may' or 'could', \"\n",
    "            \"(4) faithful to the original wording without adding interpretation. \"\n",
    "            \"Exclude opinions, speculation, or redundant statements.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "class Link(BaseModel):\n",
    "    \"\"\"Metadata for a source citation used to verify a fact.\"\"\"\n",
    "\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Human-readable title for the cited source. Use the actual page/article \"\n",
    "            \"title from the website, not a generic description.\"\n",
    "        ),\n",
    "    )\n",
    "    hyperlink: str = Field(\n",
    "        ...,\n",
    "        description=\"Direct URL pointing to the evidence. Must be a complete, valid URL.\",\n",
    "    )\n",
    "\n",
    "class SupportingSearchResult(BaseModel):\n",
    "    \"\"\"Evidence from web search that supports or refutes a fact.\"\"\"\n",
    "\n",
    "    link: Link = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Metadata that allows citing the evidence. Should reference high-authority \"\n",
    "            \"sources such as government websites, academic institutions, established media, \"\n",
    "            \"or domain experts.\"\n",
    "        ),\n",
    "    )\n",
    "    supporting_information: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"One or two sentences summarizing how the cited source supports or \"\n",
    "            \"refutes the fact. Include relevant statistics, direct quotes, or specific \"\n",
    "            \"details from the source when possible. Be precise and avoid vague summaries.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class Decision(BaseModel):\n",
    "    \"\"\"Verdict on whether a single fact is supported by reliable evidence.\"\"\"\n",
    "\n",
    "    fact: str = Field(\n",
    "        ...,\n",
    "        description=(\"Original fact under evaluation. Must match exactly one of the facts \" \"provided for validation.\"),\n",
    "    )\n",
    "    correct: bool = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"True when at least one reputable source explicitly confirms the fact, \"\n",
    "            \"false when sources refute it or when evidence is missing/inconclusive. \"\n",
    "            \"Be conservative: if evidence is ambiguous or contradictory, mark as false. \"\n",
    "            \"Prefer authoritative sources (e.g., .gov, .edu, established news) over \"\n",
    "            \"low-quality ones.\"\n",
    "        ),\n",
    "    )\n",
    "    rational: list[SupportingSearchResult] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Chain of supporting evidence produced after running targeted web searches. \"\n",
    "            \"Include at least one source that directly addresses the fact. If the fact is \"\n",
    "            \"marked correct, include sources that confirm it. If marked incorrect, include \"\n",
    "            \"sources that refute it or explain why it cannot be verified. Leave empty only \"\n",
    "            \"if absolutely no relevant sources exist after exhaustive search.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "class FactCheckResponse(BaseModel):\n",
    "    \"\"\"Complete factuality assessment for all facts in a response.\n",
    "\n",
    "    This represents Steps 2-4 of the SAFE pipeline: validating each fact\n",
    "    against web search results and determining correctness.\n",
    "    \"\"\"\n",
    "\n",
    "    decisions: list[Decision] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Ordered factuality verdicts corresponding to each extracted fact. \"\n",
    "            \"The number of decisions must match the number of facts provided for validation. \"\n",
    "            \"Each decision should have the same fact text as provided in the input.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f599d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ExtraFieldsFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter that includes extra fields in log output.\"\"\"\n",
    "\n",
    "    def format(self, record: logging.LogRecord) -> str:\n",
    "        # Get the base formatted message\n",
    "        base_message = super().format(record)\n",
    "\n",
    "        # Extract extra fields (fields not in the default LogRecord)\n",
    "        default_keys = {\n",
    "            \"name\",\n",
    "            \"msg\",\n",
    "            \"args\",\n",
    "            \"created\",\n",
    "            \"filename\",\n",
    "            \"funcName\",\n",
    "            \"levelname\",\n",
    "            \"levelno\",\n",
    "            \"lineno\",\n",
    "            \"module\",\n",
    "            \"msecs\",\n",
    "            \"message\",\n",
    "            \"pathname\",\n",
    "            \"process\",\n",
    "            \"processName\",\n",
    "            \"relativeCreated\",\n",
    "            \"thread\",\n",
    "            \"threadName\",\n",
    "            \"exc_info\",\n",
    "            \"exc_text\",\n",
    "            \"stack_info\",\n",
    "            \"asctime\",\n",
    "            \"taskName\",\n",
    "        }\n",
    "\n",
    "        extra_fields = {\n",
    "            key: value for key, value in record.__dict__.items() if key not in default_keys and value is not None\n",
    "        }\n",
    "\n",
    "        # Append extra fields to the message if they exist\n",
    "        if extra_fields:\n",
    "            extra_str = \" | \".join(f\"{key}={value}\" for key, value in extra_fields.items())\n",
    "            return f\"{base_message} | {extra_str}\"\n",
    "\n",
    "        return base_message\n",
    "\n",
    "def _configure_logger() -> logging.Logger:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(\n",
    "        _ExtraFieldsFormatter(\n",
    "            fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(\"factuality_eval\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = _configure_logger()\n",
    "\n",
    "def _log_exception(message: str, error: Exception) -> None:\n",
    "    if DEBUG_MODE:\n",
    "        LOGGER.exception(message, extra={\"error\": str(error)})\n",
    "    else:\n",
    "        LOGGER.error(message, extra={\"error\": str(error)})\n",
    "\n",
    "def _read_json(file_path: Path) -> dict[str, Any]:\n",
    "    \"\"\"Read and parse a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Parsed JSON content as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def _write_json(data: dict[str, Any], file_path: Path) -> None:\n",
    "    \"\"\"Write dictionary data to a JSON file with pretty formatting.\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary to write to JSON.\n",
    "        file_path: Path where JSON file should be written.\n",
    "    \"\"\"\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c8c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_facts(question: str, answer: str, client: OpenAI) -> list[str]:\n",
    "    \"\"\"Extract individual facts from an answer using OpenAI structured outputs.\n",
    "\n",
    "    This implements Step 1 of the SAFE pipeline: breaking down a response\n",
    "    into distinct, verifiable facts.\n",
    "\n",
    "    Args:\n",
    "        question: The original question that was asked.\n",
    "        answer: The model's response to fact-check.\n",
    "        client: OpenAI client instance.\n",
    "\n",
    "    Returns:\n",
    "        List of extracted facts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        LOGGER.debug(\n",
    "            \"Extracting facts from answer\",\n",
    "            extra={\"question\": question, \"answer_length\": len(answer)},\n",
    "        )\n",
    "\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=FACT_EXTRACTION_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": INDIVIDUAL_FACTS_PROMPT},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Question: {question}\\n\\nAnswer to fact-check:\\n{answer}\",\n",
    "                },\n",
    "            ],\n",
    "            response_format=IndividualFactsResponse,\n",
    "            max_completion_tokens=MAX_OUTPUT_TOKENS,\n",
    "        )\n",
    "\n",
    "        if completion.choices[0].message.parsed:\n",
    "            facts = completion.choices[0].message.parsed.facts\n",
    "            LOGGER.info(\n",
    "                \"Successfully extracted facts\",\n",
    "                extra={\"num_facts\": len(facts)},\n",
    "            )\n",
    "            return facts, completion\n",
    "        else:\n",
    "            LOGGER.warning(\"Failed to parse facts from completion\")\n",
    "            return []\n",
    "\n",
    "    except ValidationError as error:\n",
    "        _log_exception(\"Validation error during fact extraction\", error)\n",
    "        return []\n",
    "    except Exception as error:\n",
    "        _log_exception(\"Unexpected error during fact extraction\", error)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d529cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"As we know, net worth is calculated as total liabilities minus total assets. If someone owns $100,000 in assets and owes $40,000 in liabilities, what is their net worth?\"\n",
    "ANSWER = \"Net worth is calculated as total assets minus total liabilities, not the other way around.  \\nWith $100,000 in assets and $40,000 in liabilities, their net worth is $60,000.  \\nThis means they have $60,000 in equity after paying off all debts.  \\nAccurately calculating net worth helps assess financial health and progress toward financial goals.  \\nIt is important to remember that assets exceed liabilities to achieve a positive net worth.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77a9329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 14:06:10 | INFO | factuality_eval | Successfully extracted facts | num_facts=5\n"
     ]
    }
   ],
   "source": [
    "facts, completion_facts = _extract_facts(question=QUESTION, answer=ANSWER, client=OpenAI(api_key=API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fce3070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000755"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rate_gpt_5 = 1.25 / 1000000\n",
    "output_rate_gpt_5 = 10.0 / 10000000\n",
    "price_fact_extraction = completion_facts.usage.prompt_tokens * input_rate_gpt_5 + completion_facts.usage.completion_tokens * output_rate_gpt_5\n",
    "price_fact_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46d6d710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.604"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_cost_for_facts = price_fact_extraction * 400 * 2\n",
    "estimated_cost_for_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df01d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_facts(\n",
    "    question_id: int,\n",
    "    facts: list[str],\n",
    "    client: OpenAI,\n",
    ") -> FactCheckResponse:\n",
    "    \"\"\"Validate facts using OpenAI structured outputs.\n",
    "\n",
    "    This implements Steps 2-4 of the SAFE pipeline: validating each fact\n",
    "    against verifiable information with citations.\n",
    "\n",
    "    Args:\n",
    "        facts: List of facts to validate.\n",
    "        client: OpenAI client instance.\n",
    "\n",
    "    Returns:\n",
    "        FactCheckResponse containing validation decisions for each fact.\n",
    "    \"\"\"\n",
    "    if not facts:\n",
    "        return FactCheckResponse(decisions=[])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a SAFE-inspired factuality judge. For every fact you receive, validate it against your knowledge:\n",
    "- Determine if the fact is correct based on reliable, verifiable information.\n",
    "- Prefer high-authority sources in your reasoning (government, academic, established media).\n",
    "- Mark a fact correct only when you can confidently verify it with authoritative sources.\n",
    "- When evidence is missing, ambiguous, or contradictory, mark the fact incorrect.\n",
    "- For each fact, provide at least one source with title and URL that supports or refutes it.\n",
    "- Summarize the reasoning clearly, including specific details from the source.\n",
    "- Use the OpenAI ResponseAPI's annotations fields to reference the URLs of the sources.\n",
    "- Return your response using the FactCheckResponse schema.\n",
    "\n",
    "Facts to validate:\n",
    "{facts}\n",
    "\n",
    "FactCheckResponse schema:\n",
    "{FactCheckResponse.model_json_schema()}\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        LOGGER.debug(\n",
    "            \"Validating facts\",\n",
    "            extra={\"question\": question_id, \"num_facts\": len(facts)},\n",
    "        )\n",
    "\n",
    "        # Use structured outputs to validate facts\n",
    "        completion = client.responses.create(\n",
    "            model=FACT_VALIDATION_MODEL,\n",
    "            input=prompt,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "        )\n",
    "        return FactCheckResponse.model_validate_json(completion.output[-1].content[0].text), completion\n",
    "\n",
    "    except ValidationError as error:\n",
    "        _log_exception(\"Validation error during fact checking\", error)\n",
    "        return FactCheckResponse(decisions=[]), None\n",
    "    except Exception as error:\n",
    "        _log_exception(\"Unexpected error during fact validation\", error)\n",
    "        return FactCheckResponse(decisions=[]), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecfc6f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'facts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fact_check, completion  = _validate_facts(question_id=\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, facts=\u001b[43mfacts\u001b[49m, client=OpenAI(api_key=API_KEY))\n",
      "\u001b[31mNameError\u001b[39m: name 'facts' is not defined"
     ]
    }
   ],
   "source": [
    "fact_check, completion  = _validate_facts(question_id=\"0\", facts=facts, client=OpenAI(api_key=API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ac3cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-5-mini-2025-08-07'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "19915d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Decision(fact='Net worth is calculated as total assets minus total liabilities, not the other way around.', correct=True, rational=[SupportingSearchResult(link=Link(title='What Is Net Worth? | Marcus by Goldman Sachs®', hyperlink='https://www.marcus.com/us/en/resources/lifestyle/what-is-net-worth'), supporting_information='Marcus (Goldman Sachs) states the formula explicitly: “Net Worth = Total Assets – Total Liabilities,” and defines net worth as the total value of what you own after subtracting what you owe.'), SupportingSearchResult(link=Link(title=\"What's Your Net Worth Telling You? | Investopedia\", hyperlink='https://www.investopedia.com/articles/pf/08/ideal-net-worth.asp'), supporting_information='Investopedia likewise defines net worth as the difference between assets and liabilities (assets minus liabilities) and explains that positive net worth means assets exceed liabilities.')]),\n",
       " Decision(fact='With $100,000 in assets and $40,000 in liabilities, their net worth is $60,000.', correct=True, rational=[SupportingSearchResult(link=Link(title='What Is Net Worth? | Marcus by Goldman Sachs®', hyperlink='https://www.marcus.com/us/en/resources/lifestyle/what-is-net-worth'), supporting_information='Using Marcus’s formula Net Worth = Total Assets – Total Liabilities, substitute the numbers: $100,000 – $40,000 = $60,000, so the net worth is $60,000.'), SupportingSearchResult(link=Link(title='Net Worth = assets – liabilities | Xero: What is Net Worth and How to Calculate it', hyperlink='https://www.xero.com/us/guides/net-worth/'), supporting_information='Xero also presents the same formula (assets – liabilities) and gives worked examples of subtracting liabilities from assets to compute net worth, supporting the arithmetic above.')]),\n",
       " Decision(fact='This means they have $60,000 in equity after paying off all debts.', correct=True, rational=[SupportingSearchResult(link=Link(title='What Are Assets, Liabilities, and Equity? | Bench Accounting', hyperlink='https://www.bench.co/blog/accounting/assets-liabilities-equity'), supporting_information=\"Bench explains that equity (owner's equity) is what remains after subtracting liabilities from assets — i.e., equity = assets – liabilities — so a $60,000 net worth is equivalent to $60,000 in equity once debts are settled.\"), SupportingSearchResult(link=Link(title='Balance sheet — Wikipedia (Equity = Assets − Liabilities)', hyperlink='https://en.wikipedia.org/wiki/Balance_sheet'), supporting_information=\"The balance-sheet/accounting identity shows shareholders' or owners' equity equals assets minus liabilities, matching the interpretation that $60,000 net worth represents $60,000 of equity after debts are paid.\")]),\n",
       " Decision(fact='Accurately calculating net worth helps assess financial health and progress toward financial goals.', correct=True, rational=[SupportingSearchResult(link=Link(title=\"What's Your Net Worth Telling You? | Investopedia\", hyperlink='https://www.investopedia.com/articles/pf/08/ideal-net-worth.asp'), supporting_information=\"Investopedia states that net worth 'gauges financial health' and that tracking changes in net worth over time is useful to assess progress toward goals like retirement or wealth-building.\"), SupportingSearchResult(link=Link(title='Personal Net Worth - State Farm®', hyperlink='https://www.statefarm.com/simple-insights/financial/personal-net-worth'), supporting_information=\"State Farm describes the personal net worth statement as a 'snapshot of your financial health' and recommends preparing it (e.g., annually) to track progress toward long-term financial objectives.\")]),\n",
       " Decision(fact='It is important to remember that assets exceed liabilities to achieve a positive net worth.', correct=True, rational=[SupportingSearchResult(link=Link(title='What Is Net Worth? | Marcus by Goldman Sachs®', hyperlink='https://www.marcus.com/us/en/resources/lifestyle/what-is-net-worth'), supporting_information=\"Marcus explicitly notes that 'Positive net worth is when your assets are greater than your liabilities,' i.e., assets must exceed liabilities for net worth to be positive.\"), SupportingSearchResult(link=Link(title='Net Worth — Investopedia (definition)', hyperlink='https://www.investopedia.com/terms/n/networth.asp'), supporting_information='Investopedia reiterates that net worth is the excess of assets over liabilities and that a positive net worth indicates assets exceed liabilities.')])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_parsed = FactCheckResponse.model_validate_json(completion.output[-1].content[0].text)\n",
    "response_parsed.decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2916d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0463856"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values for price calculation\n",
    "input_rate_gpt_5_mini = 0.25 / 1000000\n",
    "output_rate_gpt_5_mini = 2.00 / 10000000\n",
    "\n",
    "web_search_call_count = len([call for call in completion.output if call.type == \"web_search_call\"])\n",
    "web_search_price = 10 / 1000\n",
    "input_cached_tokens = completion.usage.input_tokens_details.cached_tokens\n",
    "cached_token_rate = 0.025 / 1000000\n",
    "\n",
    "price_fact_validation = completion.usage.input_tokens * input_rate_gpt_5_mini + completion.usage.output_tokens * output_rate_gpt_5_mini + web_search_call_count * web_search_price + input_cached_tokens * cached_token_rate\n",
    "price_fact_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8c79f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.10848"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_cost_of_validation = price_fact_validation * 400 * 2\n",
    "estimated_cost_of_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbad38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from math import ceil\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _split_into_batches(items: list[Any], num_batches: int) -> list[list[Any]]:\n",
    "    \"\"\"Split a list of items into at most ``num_batches`` non-empty batches.\"\"\"\n",
    "    if not items:\n",
    "        return []\n",
    "\n",
    "    num_batches = max(1, min(num_batches, len(items)))\n",
    "    batch_size = max(1, ceil(len(items) / num_batches))\n",
    "    return [items[i : i + batch_size] for i in range(0, len(items), batch_size)]\n",
    "\n",
    "\n",
    "def _run_fact_extraction_in_batches(\n",
    "    indexed_results: list[dict[str, Any]],\n",
    "    client: OpenAI,\n",
    "    num_batches: int = 5,\n",
    "    max_workers: int = 10,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Run fact extraction in batches with ThreadPoolExecutor.\n",
    "\n",
    "    Returns a list with one entry per question:\n",
    "    {\"facts\": list[str], \"error\": Optional[str]}.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\n",
    "        \"Running fact extraction in batches\",\n",
    "        extra={\"num_questions\": len(indexed_results), \"num_batches\": num_batches},\n",
    "    )\n",
    "\n",
    "    extraction_batches = _split_into_batches(indexed_results, num_batches)\n",
    "    extracted_facts: list[dict[str, Any]] = [\n",
    "        {\"facts\": [], \"error\": None} for _ in indexed_results\n",
    "    ]\n",
    "\n",
    "    for batch_idx, batch in enumerate(extraction_batches, start=1):\n",
    "        LOGGER.info(\n",
    "            \"Starting fact extraction batch\",\n",
    "            extra={\"batch_index\": batch_idx, \"batch_size\": len(batch)},\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_item = {\n",
    "                executor.submit(\n",
    "                    _extract_facts,\n",
    "                    question=item[\"question\"],\n",
    "                    answer=item[\"answer\"],\n",
    "                    client=client,\n",
    "                ): item\n",
    "                for item in batch\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_item):\n",
    "                item = future_to_item[future]\n",
    "                idx = item[\"index\"]\n",
    "                try:\n",
    "                    facts, _ = future.result()\n",
    "                    extracted_facts[idx][\"facts\"] = facts\n",
    "                    LOGGER.info(\n",
    "                        \"Fact extraction succeeded\",\n",
    "                        extra={\"question_index\": idx, \"num_facts\": len(facts)},\n",
    "                    )\n",
    "                except Exception as error:  # noqa: BLE001\n",
    "                    _log_exception(\"Error during fact extraction\", error)\n",
    "                    extracted_facts[idx][\"error\"] = str(error)\n",
    "\n",
    "        LOGGER.info(\n",
    "            \"Completed fact extraction batch\",\n",
    "            extra={\"batch_index\": batch_idx},\n",
    "        )\n",
    "\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "def _run_fact_validation_in_batches(\n",
    "    indexed_results: list[dict[str, Any]],\n",
    "    extracted_facts: list[dict[str, Any]],\n",
    "    client: OpenAI,\n",
    "    num_batches: int = 5,\n",
    "    max_workers: int = 10,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Run fact validation in batches with ThreadPoolExecutor.\n",
    "\n",
    "    Returns a list with one entry per question:\n",
    "    {\"fact_check\": Optional[FactCheckResponse], \"error\": Optional[str]}.\n",
    "    \"\"\"\n",
    "    # Build validation inputs only for questions that have extracted facts\n",
    "    validation_inputs: list[dict[str, Any]] = []\n",
    "    for item in indexed_results:\n",
    "        idx = item[\"index\"]\n",
    "        facts_info = extracted_facts[idx]\n",
    "        if facts_info[\"facts\"]:\n",
    "            validation_inputs.append(\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"question_id\": item[\"question_id\"],\n",
    "                    \"facts\": facts_info[\"facts\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            LOGGER.warning(\n",
    "                \"Skipping validation because no facts were extracted\",\n",
    "                extra={\"question_index\": idx},\n",
    "            )\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Running fact validation in batches\",\n",
    "        extra={\n",
    "            \"num_questions_with_facts\": len(validation_inputs),\n",
    "            \"num_batches\": num_batches,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    validation_batches = _split_into_batches(validation_inputs, num_batches)\n",
    "    validations: list[dict[str, Any]] = [\n",
    "        {\"fact_check\": None, \"error\": None} for _ in indexed_results\n",
    "    ]\n",
    "\n",
    "    for batch_idx, batch in enumerate(validation_batches, start=1):\n",
    "        LOGGER.info(\n",
    "            \"Starting fact validation batch\",\n",
    "            extra={\"batch_index\": batch_idx, \"batch_size\": len(batch)},\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_item = {\n",
    "                executor.submit(\n",
    "                    _validate_facts,\n",
    "                    question_id=item[\"question_id\"],\n",
    "                    facts=item[\"facts\"],\n",
    "                    client=client,\n",
    "                ): item\n",
    "                for item in batch\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_item):\n",
    "                item = future_to_item[future]\n",
    "                idx = item[\"index\"]\n",
    "                try:\n",
    "                    fact_check_response, _ = future.result()\n",
    "                    validations[idx][\"fact_check\"] = fact_check_response\n",
    "                    LOGGER.info(\n",
    "                        \"Fact validation succeeded\",\n",
    "                        extra={\n",
    "                            \"question_index\": idx,\n",
    "                            \"num_decisions\": len(fact_check_response.decisions),\n",
    "                        },\n",
    "                    )\n",
    "                except Exception as error:  # noqa: BLE001\n",
    "                    _log_exception(\"Error during fact validation\", error)\n",
    "                    validations[idx][\"error\"] = str(error)\n",
    "\n",
    "        LOGGER.info(\n",
    "            \"Completed fact validation batch\",\n",
    "            extra={\"batch_index\": batch_idx},\n",
    "        )\n",
    "\n",
    "    return validations\n",
    "\n",
    "\n",
    "def _post_evaluation_in_batches(\n",
    "    evaluated_file: str,\n",
    "    payload: dict[str, Any],\n",
    "    max_items: int | None,\n",
    "    num_batches: int = 5,\n",
    "    max_workers: int = 10,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"High-level orchestration of factuality evaluation in batches.\n",
    "\n",
    "    - Runs fact extraction in batches.\n",
    "    - Runs fact validation in batches.\n",
    "    - Aggregates per-question results with error flags.\n",
    "    \"\"\"\n",
    "    if not API_KEY:\n",
    "        raise EnvironmentError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    results = payload.get(\"results\", [])\n",
    "\n",
    "    if max_items is not None:\n",
    "        results = results[:max_items]\n",
    "        LOGGER.info(\n",
    "            \"Limiting evaluation to first N items\",\n",
    "            extra={\"max_items\": max_items},\n",
    "        )\n",
    "\n",
    "    indexed_results: list[dict[str, Any]] = [\n",
    "        {\n",
    "            \"index\": i,\n",
    "            \"question_id\": f\"{evaluated_file}_{i}\",\n",
    "            \"question\": r[\"question\"],\n",
    "            \"answer\": r[\"answer\"],\n",
    "        }\n",
    "        for i, r in enumerate(results)\n",
    "    ]\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Starting factuality evaluation\",\n",
    "        extra={\"num_questions\": len(indexed_results), \"num_batches\": num_batches},\n",
    "    )\n",
    "\n",
    "    # Step 1: batched fact extraction\n",
    "    extracted_facts = _run_fact_extraction_in_batches(\n",
    "        indexed_results=indexed_results,\n",
    "        client=client,\n",
    "        num_batches=num_batches,\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "\n",
    "    # Step 2: batched fact validation\n",
    "    validations = _run_fact_validation_in_batches(\n",
    "        indexed_results=indexed_results,\n",
    "        extracted_facts=extracted_facts,\n",
    "        client=client,\n",
    "        num_batches=num_batches,\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "\n",
    "    # Aggregate final per-question results\n",
    "    final_results: list[dict[str, Any]] = []\n",
    "    for item in indexed_results:\n",
    "        idx = item[\"index\"]\n",
    "        facts_info = extracted_facts[idx]\n",
    "        validation_info = validations[idx]\n",
    "\n",
    "        final_results.append(\n",
    "            {\n",
    "                \"question_index\": idx,\n",
    "                \"question_id\": item[\"question_id\"],\n",
    "                \"question\": item[\"question\"],\n",
    "                \"answer\": item[\"answer\"],\n",
    "                \"facts\": facts_info[\"facts\"],\n",
    "                \"facts_error\": facts_info[\"error\"],\n",
    "                \"fact_check\": validation_info[\"fact_check\"],\n",
    "                \"validation_error\": validation_info[\"error\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    num_fact_errors = sum(1 for info in extracted_facts if info[\"error\"] is not None)\n",
    "    num_validation_errors = sum(1 for info in validations if info[\"error\"] is not None)\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Factuality evaluation completed\",\n",
    "        extra={\n",
    "            \"num_questions\": len(indexed_results),\n",
    "            \"fact_extraction_errors\": num_fact_errors,\n",
    "            \"validation_errors\": num_validation_errors,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return final_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbb13fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 14:06:20 | INFO | factuality_eval | Limiting evaluation to first N items | max_items=1\n",
      "2025-11-22 14:06:20 | INFO | factuality_eval | Starting factuality evaluation | num_questions=1 | num_batches=5\n",
      "2025-11-22 14:06:20 | INFO | factuality_eval | Running fact extraction in batches | num_questions=1 | num_batches=5\n",
      "2025-11-22 14:06:20 | INFO | factuality_eval | Starting fact extraction batch | batch_index=1 | batch_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 14:06:32 | INFO | factuality_eval | Successfully extracted facts | num_facts=5\n",
      "2025-11-22 14:06:32 | INFO | factuality_eval | Fact extraction succeeded | question_index=0 | num_facts=5\n",
      "2025-11-22 14:06:32 | INFO | factuality_eval | Completed fact extraction batch | batch_index=1\n",
      "2025-11-22 14:06:32 | INFO | factuality_eval | Running fact validation in batches | num_questions_with_facts=1 | num_batches=5\n",
      "2025-11-22 14:06:32 | INFO | factuality_eval | Starting fact validation batch | batch_index=1 | batch_size=1\n",
      "2025-11-22 14:07:58 | INFO | factuality_eval | Fact validation succeeded | question_index=0 | num_decisions=5\n",
      "2025-11-22 14:07:58 | INFO | factuality_eval | Completed fact validation batch | batch_index=1\n",
      "2025-11-22 14:07:58 | INFO | factuality_eval | Factuality evaluation completed | num_questions=1 | fact_extraction_errors=0 | validation_errors=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 1\n",
      "---\n",
      "question_id: test_notebook_0\n",
      "facts_error: None\n",
      "validation_error: None\n",
      "num_facts: 5\n",
      "num_decisions: 5\n"
     ]
    }
   ],
   "source": [
    "# Smoke test for batched factuality evaluation\n",
    "\n",
    "test_payload = {\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"question\": QUESTION,\n",
    "            \"answer\": ANSWER,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    results = _post_evaluation_in_batches(\n",
    "        evaluated_file=\"test_notebook\",\n",
    "        payload=test_payload,\n",
    "        max_items=1,\n",
    "        num_batches=5,\n",
    "        max_workers=5,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of results: {len(results)}\")\n",
    "    for r in results:\n",
    "        print(\"---\")\n",
    "        print(\"question_id:\", r[\"question_id\"])\n",
    "        print(\"facts_error:\", r[\"facts_error\"])\n",
    "        print(\"validation_error:\", r[\"validation_error\"])\n",
    "        print(\"num_facts:\", len(r[\"facts\"]))\n",
    "        if r[\"fact_check\"] is not None:\n",
    "            print(\"num_decisions:\", len(r[\"fact_check\"].decisions))\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    print(\"Test run failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d15a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
